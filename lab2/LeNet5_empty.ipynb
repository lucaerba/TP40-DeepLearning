{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iafPdtuncbq7"
      },
      "source": [
        "<h2><center>MNIST classification using <i>LeNet5</i></center></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4VrCB5La5rD"
      },
      "source": [
        "# Importing Keras\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlKZ3Hnas7B4"
      },
      "source": [
        "# Importing the Keras 2.x main module relying on tensorflow 2.x backend\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "print(\"Using tensorflow version \" + str(tf.__version__))\n",
        "print(\"Using keras version \" + str(keras.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_QLz9_jbRZq"
      },
      "source": [
        "# Loading and preparing the MNIST dataset\n",
        "Load the MNIST dataset made available by keras.datasets\n",
        "Verify the amount of system memory available before and after loading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG83hGyVmijn"
      },
      "source": [
        "#@title\n",
        "# The MNSIT dataset is ready to be imported from Keras into RAM\n",
        "# Warning: you cannot do that for larger databases (e.g., ImageNet\n",
        "from keras.datasets import ...\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqtZCOxcDS0U"
      },
      "source": [
        "Turn train and test labels to one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQbkllF8mnaf"
      },
      "source": [
        "# Turning the lables into one-hot format\n",
        "from keras.utils.np_utils import to_categorical\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTSbzJ8zDi0b"
      },
      "source": [
        "Reshape train and test images so that they follow the NWHC ordering required by the TF backend. Then, after casting the pixels to floats, normalize the images so that they have zero-mean and unitary deviation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptTRSDo5nJyZ"
      },
      "source": [
        "# Reshaping the images to NWHC scheme\n",
        "img_rows, img_cols = train_images.shape[1], train_images.shape[2]\n",
        "train_images = train_images.reshape(...)\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "\n",
        "# Cast pixels from uint8 to float32\n",
        "train_images = train_images.astype('float32')\n",
        "\n",
        "# Now let us normalize the images so that they have zero mean and standard deviation\n",
        "# Hint: are real testing data statistics known at training time ?\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwm1OFOtc4uU"
      },
      "source": [
        "# Defining the neural network architecture (i.e., the network model)\n",
        "Create a LeNet5-like convolutional neural network taking in input the images as matrices of pixels and suitable to classify each image across 10 different classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnd3q1V3nk8v"
      },
      "source": [
        "# The Sequential module is sort of a container for more complex NN elements and\n",
        "# defines a loop-less NN architecture\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution2D, MaxPooling2D\n",
        "from keras.layers import ..., ...\n",
        "\n",
        "input_shape = (..., ..., ...)\n",
        "output_shape = ...\n",
        "\n",
        "# size of pooling area for max pooling\n",
        "pool_size = (..., ...)\n",
        "# convolution kernel size\n",
        "kernel_size = (..., ...)\n",
        "# Number of filters in first convolutional layer\n",
        "num_kernel_first_conv_layer = ...\n",
        "# Number of filters in second convolutional layer\n",
        "num_kernel_second_conv_layer = ...\n",
        "\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_QEHUznSMyY"
      },
      "source": [
        "Instantiate a SGD optimizer with a tentative LR of 10^-2 and using the appropriate loss function and compile the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ParUs_WKSNmZ"
      },
      "source": [
        "# The optimizers module provides a number of optimization algorithms for updating\n",
        "# a netwok parameters accoridng to the computed error gradints\n",
        "from keras import optimizers\n",
        "\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "# We can now have a look at the defined model topology\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the network - fit()"
      ],
      "metadata": {
        "id": "GZjKqhNjM0cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is where the actual training-testing happens\n",
        "# Dimension of the batch size (number of images over which error gradients are averaged)\n",
        "batch_size = 20\n",
        "# Number of epochs we want to train\n",
        "epochs = 10\n",
        "# We restrict the training to 10k images to cut time\n",
        "history=model.fit(...)"
      ],
      "metadata": {
        "id": "HNGl2cKXM1_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "POxbmUJzM0GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AWUAW4idF3D"
      },
      "source": [
        "# Training the network - train_on_batch()\n",
        "Train the model for 10 epochs using the train_on_batch() and test_on_batch() methods, manually cycling through generations and batches od samples.\n",
        "Hint 1: instantiate a ImageDataGenerator() to get an iterator over the samples.\n",
        "Hint 2: store the per-epoch test and train loss and accuracy to plot them later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTHrbb7uFYWz"
      },
      "source": [
        "# This is where the actual training-testing happens\n",
        "# Dimension of the batch size (number of images over which error gradients are averaged)\n",
        "batch_size = 100\n",
        "# Number of epochs we want to train\n",
        "epochs = 10\n",
        "\n",
        "#For being able to plot the training history afterwards\n",
        "history = {}\n",
        "history['loss'] = []\n",
        "history['val_loss'] = []\n",
        "history['acc'] = []\n",
        "history['val_acc'] = []\n",
        "\n",
        "\n",
        "# Creating a batch preprocessor for augmenting the trainig data\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# START CODE HERE\n",
        "myDatagen = ...\n",
        "...\n",
        "# END CODE HERE\n",
        "\n",
        "# Cycling through the epochs\n",
        "for e in range(epochs):\n",
        "    lossEpochTrain = 0\n",
        "    lossEpochTest = 0\n",
        "    accuracyEpochTrain = 0\n",
        "    accuracyEpochTest = 0\n",
        "\n",
        "    # Training over the training samlpes, batch by batch\n",
        "    batchCnt = 0\n",
        "    for images_batch, labels_batch in myDatagen.flow(train_images, train_labels, batch_size=batch_size):\n",
        "        # START CODE HERE\n",
        "        ...\n",
        "        # END CODE HERE\n",
        "\n",
        "    # Testing over the training samlpes, batch by batch\n",
        "    batchCnt = 0\n",
        "    for images_batch, labels_batch in myDatagen.flow(test_images, test_labels, batch_size=batch_size):\n",
        "        # START CODE HERE\n",
        "        ...\n",
        "        # END CODE HERE\n",
        "\n",
        "    print ('Epoch %d / %d lossTrain %.3f lossTest %.3f accuracyTrain %.3f accuracyTest %.3f' %(int(e), epochs, lossEpochTrain/batchCnt, lossEpochTest/batchCnt, accuracyEpochTrain/batchCnt, accuracyEpochTest/batchCnt))\n",
        "    history['loss'].append(lossEpochTrain/batchCnt)\n",
        "    history['val_loss'].append(lossEpochTest/batchCnt)\n",
        "    history['acc'].append(accuracyEpochTrain/batchCnt)\n",
        "    history['val_acc'].append(accuracyEpochTest/batchCnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODUc5Bq_dMEq"
      },
      "source": [
        "# Visualizing the network performance\n",
        "Visualize the training history using the pyplot package: plot in one graph the train and vaidation loss functions, in another graph the train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdJrRbyariEw"
      },
      "source": [
        "# We now want to plot the train and validation loss functions and accuracy curves\n",
        "\n",
        "# summarize history for loss\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "plt.show()\n",
        "\n",
        "# summarize history for accuracy\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr4TdWoEoDzi"
      },
      "source": [
        "# Experiments\n",
        "\n",
        "Note down the performance of the trained network in terms of training and validation accuracy as a reference. Then, experiment as follow and compare performance with the reference scenario.\n",
        "\n",
        "*   **Hidden layers activations**: experiment replacing sigmoids with ReLUs.\n",
        "*   **Filter size**: experiment with (stacks of) square filters of size 3x3 so to obtain equivalent filters 5x5.\n",
        "*   **Number of filters**: experiment increasing the number of filters in the first and second layer and find the maximum number of filters the network can tolerate before overfitting to the training samples.\n",
        "* **Pooling-less architectures**: Modify the network architecture to obtain a twofold reduction of each featuremap without resorting to pooling layers (hint: take insipiration from ResNet).\n",
        "* **Confusion analysis**: Using the proper metric  from sklearn, check which character is most frequently confused with which: can you explain why ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0C1J6R1Elfc"
      },
      "source": [
        "# Example of a confusion matrix using sklearn.metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "predictions = model.predict(...)\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "print (matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh_WVRm_fMXP"
      },
      "source": [
        "#Saving the training results\n",
        "\n",
        "Save the best trained model (topology, parameters), and all the related side information required to deploy the trained model later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbJCJZ3mfOYu"
      },
      "source": [
        "# Create a directory for saving both the trained model and side information\n",
        "import os\n",
        "save_dir = os.path.join(os.getcwd(), 'trained_lenet5_mnist')\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# Save model and weights\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Saving mean and standard deviation information as a CSV file\n",
        "import csv\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "print('Saved side information at %s ' % model_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}