{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet300_empty.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iafPdtuncbq7"
      },
      "source": [
        "<h2><center>MNIST classification using <i>LeNet300</i></center></h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4VrCB5La5rD"
      },
      "source": [
        "# Importing Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlKZ3Hnas7B4"
      },
      "source": [
        "# Importing the Keras 2.x main module relying on tensorflow 2.x backend\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "print(\"Using tensorflow version \" + str(tf.__version__))\n",
        "print(\"Using keras version \" + str(keras.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_QLz9_jbRZq"
      },
      "source": [
        "# Loading and preparing the MNIST dataset\n",
        "Load the MNIST dataset made available by keras.datasets\n",
        "Verify the amount of system memory available before and after loading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG83hGyVmijn"
      },
      "source": [
        "#@title\n",
        "# The MNSIT dataset is ready to be imported from Keras into RAM\n",
        "from keras.datasets import ...\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRPbU_Z4U6Ac"
      },
      "source": [
        "Using the pyplot package, visualize the fist sample of the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5VAu7oW0Zu4"
      },
      "source": [
        "# Let us visualize the first training sample using the Gnuplot library\n",
        "from matplotlib import pyplot as plt\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7YsRekMVDg-"
      },
      "source": [
        "Turn train and test labels to one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQbkllF8mnaf"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jv29YLtVO3q"
      },
      "source": [
        "Reshape train and test images so that they follow the NWHC ordering required by the TF backend.\n",
        "Then, after casting the pixels to floats, normalize the images so that they have zero-mean and unitary deviation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptTRSDo5nJyZ"
      },
      "source": [
        "# Reshape to proper images with 1 color channel according to backend scheme\n",
        "img_rows, img_cols = train_images.shape[1], train_images.shape[2]\n",
        "train_images = train_images.reshape(...)\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "\n",
        "# Cast pixels from uint8 to float32\n",
        "train_images = train_images.astype('float32')\n",
        "\n",
        "# Now let us normalize the images so that they have zero mean and standard deviation\n",
        "# Hint: testing data statistics known at training time ?\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwm1OFOtc4uU"
      },
      "source": [
        "# Defining the neural network architecture (i.e., the network model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnKYaP-KVoOs"
      },
      "source": [
        "Create a LeNet300-like fully connected network taking in input the images as vectors of pixels and suitable to classify each image across 10 different classes. For this first implementation of the network, use only sigmoid activations in the hidden layer. Which activation function shall be used for the output layer ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnd3q1V3nk8v"
      },
      "source": [
        "# The Sequential module is a container for more complex NN elements and\n",
        "# defines a loop-less NN architecture\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "\n",
        "# START CODE HERE\n",
        "input_shape = ...\n",
        "output_shape = ...\n",
        "...\n",
        "# END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g2PNEiwWBLw"
      },
      "source": [
        "Instantiate a SGD optimizer with a tentative LR of 10^-4 and using the appropriate loss function and compile the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SijGZGKjV9J2"
      },
      "source": [
        "# The optimizers module provides a number of optimization algorithms for updating\n",
        "# a netwok parameters accoridng to the computed error gradints\n",
        "# START CODE HERE\n",
        "optimizer=tf.keras.optimizers.SGD(lr= [...])\n",
        "\n",
        "# Compiling a model in Keras amounts to associating th eoptimizer to a model with an appropriate loss function \n",
        "model.compile([...])\n",
        "\n",
        "# END CODE HERE\n",
        "# We can now have a look at the defined model topology\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AWUAW4idF3D"
      },
      "source": [
        "# Training the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-WqMtSAWTRp"
      },
      "source": [
        "Train the model for 10 epochs using the fit() method, without batch training initially validating the model at each epoch and keeping track of the training history for later plotting. Make sure you enable fit() verbose mode for interactive output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTHrbb7uFYWz"
      },
      "source": [
        "# This is where the actual training-testing happens\n",
        "# Number of epochs we want to train\n",
        "epochs = 10\n",
        "\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODUc5Bq_dMEq"
      },
      "source": [
        "# Visualizing the network performance\n",
        "Visualize the training history using the pyplot package: plot in one graph the train and vaidation loss functions, in another graph the train and validation accuracy. By comparing the training the testing curves, what can we conclude about the quality of the training ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdJrRbyariEw"
      },
      "source": [
        "# We now want to plot the train and validation loss functions and accuracy curves\n",
        "#print(history.history.keys())\n",
        "\n",
        "# summarize history for loss\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "plt.show()\n",
        "\n",
        "# summarize history for accuracy\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-fHHdqYPGu9"
      },
      "source": [
        "# Computing the confusion matrix\n",
        "The confusion matrix allows to analyze the trained network performance on a per-class basis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXUXVWgLPM4R"
      },
      "source": [
        "# Example of a confusion matrix using sklearn.metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "predictions = model.predict(test_images)\n",
        "# Mind that confusion_matrix requires\n",
        "matrix = confusion_matrix(test_labels.argmax(axis=1), predictions.argmax(axis=1))\n",
        "print (matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr4TdWoEoDzi"
      },
      "source": [
        "# Experiments\n",
        "\n",
        "Note down the performance of the trained network in terms of training and validation accuracy as a reference (save the loss/accuracy graphs of the trained network).\n",
        "\n",
        "Then,  experiment as follow and compare performance with the reference scenario:\n",
        "\n",
        "*  Experiment increasing the batch size (from purely stochastic case) and compare performance with reference.\n",
        "*  Experiment gradually increasing the learning rate starting from 10^-4 and find a reasonable learning rate value the network can tolerate without diverging.\n",
        "*  Experiment replacing the sigmoid activations with Relus and note what happens.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}