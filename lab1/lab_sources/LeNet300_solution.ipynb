{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet300_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iafPdtuncbq7"
      },
      "source": [
        "<h2><center>MNIST classification using <i>LeNet300</i></center></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_48Zhg_vxTs"
      },
      "source": [
        "# As a first step, we may want to switch to a GPU-acceperated VM\n",
        "# In the menu: Runtime->Change runtime type->Hardware Accelerator->GPU.\n",
        "#\n",
        "# This will thest if we have a GPU-equipped VM and return some useful system-level information\n",
        "#!nvidia-smi\n",
        "\n",
        "# Which GNU/Linux distribution is installed on our VM ?\n",
        "#!lsb_release -a\n",
        "\n",
        "# Which version of the Linux kernel our VM has ?\n",
        "#!uname -a\n",
        "\n",
        "# How much free memory our VM has ?\n",
        "#!free -h\n",
        "\n",
        "# Which storage facilities our VM has ?\n",
        "#!mount\n",
        "\n",
        "# Which python version our VM has installed ?\n",
        "#!python --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4VrCB5La5rD"
      },
      "source": [
        "# Importing Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlKZ3Hnas7B4"
      },
      "source": [
        "# Importing the Keras 2.x main module relying on tensorflow 2.x backend\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "print(\"Using tensorflow version \" + str(tf.__version__))\n",
        "print(\"Using keras version \" + str(keras.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_QLz9_jbRZq"
      },
      "source": [
        "# Loading and preparing the MNIST dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLtK5hWxUUDD"
      },
      "source": [
        "\n",
        "Load the MNIST dataset via keras.datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG83hGyVmijn"
      },
      "source": [
        "#@title\n",
        "# The MNSIT dataset is ready to be imported from Keras into RAM\n",
        "# Warning: you cannot do that for larger databases (e.g., ImageNet)\n",
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)\n",
        "#!free -h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5VAu7oW0Zu4"
      },
      "source": [
        "# Let us visualize the first training sample using the Gnuplot library\n",
        "from matplotlib import pyplot as plt\n",
        "imageIndex = 0\n",
        "print(\"Label for \" + str(imageIndex) + \"-th train image is: \" + str(train_labels[0]))\n",
        "plt.imshow(train_images[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQbkllF8mnaf"
      },
      "source": [
        "# Labels are encode in one-hot format\n",
        "from keras.utils.np_utils import to_categorical\n",
        "#print(\"This is the native \" + str(imageIndex) + \"-th train label: \" + str(train_labels[0]))\n",
        "train_labels = to_categorical(train_labels)\n",
        "#print(\"This is the one-hot encoding of the \" + str(imageIndex) + \"-th train label: \" + str(train_labels[0]))\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptTRSDo5nJyZ"
      },
      "source": [
        "# Reshape to proper images with 1 color channel according to backend scheme\n",
        "img_rows, img_cols = train_images.shape[1], train_images.shape[2]\n",
        "train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols,1 )\n",
        "print('train_images shape:', train_images.shape)\n",
        "print('test_images shape:', test_images.shape)\n",
        "print(train_images.shape[0], 'train samples')\n",
        "print(test_images.shape[0], 'test samples')\n",
        "\n",
        "# Cast pixels from uintt8 to float32 prior to normalization\n",
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        "\n",
        "# Normalize the images so that have zero mean and unitary deviation with respect to the train set statistics\n",
        "train_mean = train_images.mean()\n",
        "train_std = train_images.std()\n",
        "train_images = (train_images - train_mean)/train_std\n",
        "test_images = (test_images - train_mean)/train_std\n",
        "\n",
        "# Alternatively, we could normalize the image in the [0-1] range instead\n",
        "#train_images = ((train_images / 255) * 2) -1\n",
        "#test_images = ((test_images / 255) * 2) -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwm1OFOtc4uU"
      },
      "source": [
        "# Defining the neural network architecture (i.e., the network model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnd3q1V3nk8v"
      },
      "source": [
        "# The Sequential module is sort of a container for more complex NN elements and\n",
        "# defines a loop-less NN architecture\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "output_shape = 10\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Flatten(input_shape=input_shape))\n",
        "model.add(Dense(300))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.add(Dense(output_shape))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TkJzPuzLx9t"
      },
      "source": [
        "Instantiating a SGD optimizer with LR = 10^-4 and printing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCK-scAzLpED"
      },
      "source": [
        "# The optimizers module provides a number of optimization algorithms for updating\n",
        "# a netwok parameters accoridng to the computed error gradients\n",
        "\n",
        "# Defining our SGD optimizer\n",
        "optimizer=tf.keras.optimizers.SGD(lr=1e-3)\n",
        "\n",
        "# Compiling a model in Keras amounts to associating th eoptimizer to a model with an appropriate loss function\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Let us have a look at the model topology\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AWUAW4idF3D"
      },
      "source": [
        "# Training the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTHrbb7uFYWz"
      },
      "source": [
        "# This is where the actual training-testing happens\n",
        "# Dimension of the batch size (number of images over which error gradients are averaged)\n",
        "batch_size = 128\n",
        "# Number of epochs we want to train\n",
        "epochs = 10\n",
        "# We restrict the training to 10k images to cut time\n",
        "n_train_samples = 60000\n",
        "history=model.fit(train_images[:n_train_samples], train_labels[:n_train_samples],\n",
        "          batch_size=batch_size, epochs=epochs,\n",
        "          verbose=1, shuffle=True, validation_data=(test_images, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xboasXH0nlj4"
      },
      "source": [
        "# We may want to independently test on a different dataset after training the network\n",
        "#score = model.evaluate(test_images, test_labels, verbose=False)\n",
        "#print('Test score:', score[0])\n",
        "#print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODUc5Bq_dMEq"
      },
      "source": [
        "# Visualizing the network performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdJrRbyariEw"
      },
      "source": [
        "# We now want to plot the train and validation loss functions and accuracy curves\n",
        "#print(history.history.keys())\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(top=3)\n",
        "plt.ylim(bottom=0)\n",
        "plt.xlabel('Epoch')\n",
        "plt.xlim(left=0)\n",
        "plt.xlim(right=10)\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(top=1)\n",
        "plt.ylim(bottom=0)\n",
        "plt.xlabel('Epoch')\n",
        "plt.xlim(left=0)\n",
        "plt.xlim(right=10)\n",
        "plt.legend(['train', 'test'], loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpNHND0fOjXq"
      },
      "source": [
        "# Computing the confusion matrix\n",
        "The confusion matrix allows to analyze the trained network performance on a per-class basis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "762xrF4AOjXs"
      },
      "source": [
        "# Example of a confusion matrix using sklearn.metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "predictions = model.predict(test_images)\n",
        "# Mind that confusion_matrix requires\n",
        "matrix = confusion_matrix(test_labels.argmax(axis=1), predictions.argmax(axis=1))\n",
        "print (matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j7gX0o-Oorz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}